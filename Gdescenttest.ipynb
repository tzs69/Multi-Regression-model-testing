{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to my simple multivariate regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read World Bank dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbdata = pd.read_csv(\"C:\\\\Users\\\\Teh Ze Shi\\\\OneDrive\\\\schoolwork\\\\BT4222\\\\WorldBankData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which takes in row and a specific column as args and fills all NA values in the specified column with the mean value of the region: \n",
    "#   e.g. country XYZ which belongs to Carribean Small States regionhas a NA value for the 'Life.Expectancy.at.birth' column \n",
    "#        -> replace NA with mean 'Life.Expectancy.at.birth' of Carribean Small States\n",
    "def fill_NAs(row, column_name):\n",
    "    if pd.isna(row[column_name]):  # Check if 'x' is NaN\n",
    "        regionCode = row['Additional'] \n",
    "        return wbdata[wbdata[\"Country.Code\"].isin([regionCode])][column_name].values[0]\n",
    "    return row[column_name]  # If 'x' is not NaN, keep the original value\n",
    "\n",
    "# Apply function to all relevant columns to be used for prediction\n",
    "wbdata['Health.Expenditure.per.capita'] = wbdata.apply(fill_NAs, axis=1, args=('Health.Expenditure.per.capita',))\n",
    "wbdata['Diabetes.Prevalence'] = wbdata.apply(fill_NAs, axis=1, args=('Diabetes.Prevalence',))\n",
    "wbdata['GDP.per.capita.PPP'] = wbdata.apply(fill_NAs, axis=1, args=('GDP.per.capita.PPP',))\n",
    "wbdata['Life.Expectancy.at.birth'] = wbdata.apply(fill_NAs, axis=1, args=('Life.Expectancy.at.birth',))\n",
    "\n",
    "# Drop rows which contain aggregated regions instead of countries/states\n",
    "noncountries = [\"ARB\", \"CSS\", \"CEB\", \"EAR\", \"EAS\", \"EAP\", \"TEA\", \"EMU\", \"ECS\", \"ECA\", \"TEC\", \"EUU\", \"FCS\", \"HPC\", \"HIC\", \"IBD\", \"IBT\", \"IDB\", \"IDX\", \"IDA\", \"LTE\", \"LCN\", \"LAC\", \"TLA\", \"LDC\", \"LMY\", \"LIC\", \"LMC\", \"MEA\", \"MNA\", \"TMN\", \"MIC\", \"NAC\", \"OED\", \"OSS\", \"PSS\", \"PST\", \"PRE\", \"SST\", \"SAS\", \"TSA\", \"SSF\", \"SSA\", \"TSS\", \"UMC\", \"WLD\"]\n",
    "wbdata = wbdata[~wbdata['Country.Code'].isin(noncountries)]\n",
    "wbdata = wbdata.reset_index(drop=True)\n",
    "\n",
    "# Randomize by shuffling rows so wbdata no longer ordered alphabetically\n",
    "wbdata = wbdata.sample(frac=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split wbdata into training(85%) and test(15%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set contains first 181 entries in wbdata (181/212 = 0.854)\n",
    "n = math.floor(0.85 * wbdata.shape[0])\n",
    "\n",
    "trainingSet0raw =wbdata.iloc[:n, [3, 4, 5]]\n",
    "trainingSet0raw_actual = wbdata.iloc[:n, 8]\n",
    "\n",
    "# Test set contains remaining rows\n",
    "testSet0raw = wbdata.iloc[n:, [3, 4, 5]]\n",
    "testSet0raw_actual = wbdata.iloc[n:, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent and Mmdel fitting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent function GDgetWeights returns optimum feature weights\n",
    "def GDgetWeights(w, learnRate, num_iterations, input_set, output_actual):\n",
    "    for _ in range(num_iterations):\n",
    "        predictions = np.matmul(input_set, w) \n",
    "        errors = predictions - output_actual\n",
    "\n",
    "        # L(w) = 1/n ǁAx - yǁ^2  --> ∇L(w) = 2/n ǁAx - yǁ\n",
    "        gradient = 2/input_set.shape[0] * np.matmul(np.transpose(errors), input_set)\n",
    "        \n",
    "        # Update wVector\n",
    "        w -= learnRate * gradient  \n",
    "        if np.linalg.norm(gradient) < 0.001:\n",
    "            print(\"Error minimized; Breaking out of loop\")\n",
    "            break\n",
    "        # print(np.linalg.norm(gradient))\n",
    "    return w\n",
    "\n",
    "# Generalized and scalable model utilizing GDgetWeights fitting function given input, returns prediction function\n",
    "def fit_model(w, learnRate, num_iterations, input_set, output_actual):   \n",
    "    # Convert input dataframes into np arrays\n",
    "    input_set, output_actual = np.array(input_set), np.array(output_actual)\n",
    "\n",
    "    # Scale input to compensate for possible extreme values of variables\n",
    "    scaler = StandardScaler()\n",
    "    input_set = scaler.fit_transform(input_set)    \n",
    "    input_set = np.hstack([np.ones((input_set.shape[0], 1)), input_set]) # Add column filled with 1s to account for bias term\n",
    "\n",
    "    # Run gradient descent to get optimum feature weights\n",
    "    weights = GDgetWeights(w, learnRate, num_iterations, input_set, output_actual)\n",
    "\n",
    "    # Prediction function\n",
    "    def predict(*independent_variables):\n",
    "        v = np.array(independent_variables).reshape(1, len(independent_variables))\n",
    "        v = scaler.transform(v)\n",
    "        v = np.column_stack([np.ones((v.shape[0], 1)), v])\n",
    "        return np.matmul(v, weights)[0]\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 - Train the model and fit it to training set trainingSet0raw\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2 - Evaluate model performance on test set testSet0raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Predicted     Actual      Error\n",
      "0   74.368697  76.009000   1.640303\n",
      "1   82.285682  81.950724  -0.334959\n",
      "2   67.770090  58.575000  -9.195090\n",
      "3   70.287665  66.766000  -3.521665\n",
      "4   70.262098  66.480000  -3.782098\n",
      "5   73.205098  71.661000  -1.544098\n",
      "6   82.467362  77.374000  -5.093362\n",
      "7   72.055121  80.030000   7.974879\n",
      "8   68.515936  62.643000  -5.872936\n",
      "9   70.371005  75.498000   5.126995\n",
      "10  77.937753  71.096018  -6.841735\n",
      "11  75.688905  73.318549  -2.370355\n",
      "12  81.771097  82.470488   0.699391\n",
      "13  70.459666  74.562000   4.102334\n",
      "14  77.586460  74.514634  -3.071825\n",
      "15  73.165293  73.318549   0.153257\n",
      "16  91.403113  82.685366  -8.717747\n",
      "17  76.968959  84.680488   7.711529\n",
      "18  66.145534  66.312000   0.166466\n",
      "19  70.510553  75.943000   5.432447\n",
      "20  67.670081  70.604000   2.933919\n",
      "21  67.756666  71.214000   3.457334\n",
      "22  65.865433  52.214000 -13.651433\n",
      "23  71.114997  78.495000   7.380003\n",
      "24  69.770269  70.565000   0.794731\n",
      "25  83.370027  81.641463  -1.728564\n",
      "26  66.077377  67.291000   1.213623\n",
      "27  65.644094  58.456000  -7.188094\n",
      "28  77.277849  81.124390   3.846541\n",
      "29  93.436994  78.331000 -15.105994\n",
      "30  66.009833  54.102000 -11.907833\n",
      "31  70.460495  73.211000   2.750505\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train model using training dataset\n",
    "\n",
    "\n",
    "# training set contains first 181 entries in wbdata (181/212 = 0.854)\n",
    "n = math.floor(0.85 * wbdata.shape[0])\n",
    "# n = 181\n",
    "# print(n)\n",
    "trainingSet0raw =wbdata.iloc[:n, [3, 4, 5]]\n",
    "trainingSet0raw_actual = wbdata.iloc[:n, 8]\n",
    "\n",
    "testSet0raw = wbdata.iloc[n:, [3, 4, 5]]\n",
    "testSet0raw_actual = wbdata.iloc[n:, 8]\n",
    "\n",
    "\n",
    "wVector = np.random.random(trainingSet0raw.shape[1] + 1) # + 1 to account for the bias weight\n",
    "lr = random.uniform(0.001, 0.01) # Initialize learning rate as a random float between 0.001 and 0.01\n",
    "\n",
    "model = fit_model(wVector, lr, 1000, trainingSet0raw, trainingSet0raw_actual)\n",
    "\n",
    "#Apply trained model on values of test dataset\n",
    "def iterator(row):\n",
    "    return model(row['Health.Expenditure.per.capita'], row['Diabetes.Prevalence'], row['GDP.per.capita.PPP'])\n",
    "\n",
    "testSet0_pred = np.array(testSet0raw.apply(iterator, axis = 1))\n",
    "testSet0raw_actual = np.array(testSet0raw_actual)\n",
    "testSet0_pred, testSet0raw_actual = testSet0_pred.reshape(-1, 1), testSet0raw_actual.reshape(-1, 1)\n",
    "\n",
    "evaluation = pd.DataFrame(np.concatenate((testSet0_pred, testSet0raw_actual), axis = 1), columns = ['Predicted', 'Actual'])\n",
    "evaluation['Error'] = evaluation['Actual'] - evaluation['Predicted']\n",
    "\n",
    "print(evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
